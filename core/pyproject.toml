[tool.poetry]
name = "leonard-core"
version = "0.1.0"
description = "The local-first engine to build and run private AI agents"
authors = ["Leonard Team"]
readme = "README.md"
packages = [{include = "leonard"}]

[tool.poetry.dependencies]
python = ">=3.11,<3.15"
# API
fastapi = "^0.115.0"
uvicorn = {extras = ["standard"], version = "^0.32.0"}
pydantic = "^2.10.0"
httpx = "^0.28.0"
# Model inference (install with Metal support on macOS)
# CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python
llama-cpp-python = "^0.3.2"
# Required for llama-cpp-python server
sse-starlette = "^2.0.0"
starlette-context = "^0.3.6"
# HuggingFace
huggingface-hub = "^0.27.0"
# RAG - LlamaIndex (local embeddings, no API keys needed)
llama-index-core = "^0.12.0"
llama-index-embeddings-huggingface = "^0.5.0"
llama-index-readers-file = "^0.4.0"

[tool.poetry.group.dev.dependencies]
pytest = "^7.4.0"
pytest-asyncio = "^0.23.0"
ruff = "^0.1.0"

[tool.poetry.scripts]
leonard = "leonard.main:app"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.ruff]
line-length = 100
target-version = "py311"

[tool.ruff.lint]
select = ["E", "F", "I", "N", "W"]
